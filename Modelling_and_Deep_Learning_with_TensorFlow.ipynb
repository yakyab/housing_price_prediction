{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Modelling and Deep Learning with TensorFlow\n",
    "\n",
    "Welcome to the third notebook of our project, where we will focus on building and training our predictive models.\n",
    "\n",
    "In this notebook, we'll be using TensorFlow, a popular open-source platform for machine learning. TensorFlow offers a comprehensive ecosystem of tools, libraries, and community resources that allows researchers and developers to build and deploy machine learning models with ease.\n",
    "\n",
    "We will start by loading the processed dataset that we created in the previous notebook. After this, we will carry out the following steps:\n",
    "\n",
    "1. **Data Splitting**: We will split our data into training and test sets. The training set will be used to train our models, while the test set will serve to evaluate their performance on unseen data.\n",
    "\n",
    "2. **Baseline Model**: We will begin with a simple linear regression model to serve as our baseline. This will allow us to gauge the performance of our subsequent, more complex models.\n",
    "\n",
    "3. **Deep Learning Model**: After establishing our baseline, we will proceed to construct a deep learning model using TensorFlow's Keras API. We will start with a basic feed-forward neural network and assess its performance.\n",
    "\n",
    "4. **Model Improvement**: We will attempt to improve the performance of our deep learning model by tuning its architecture and hyperparameters. We may include techniques such as adding more layers, using different types of layers (like dropout for regularization), or adjusting the learning rate.\n",
    "\n",
    "5. **Model Training**: Our models will be trained on our training set, using the appropriate loss functions and optimizers for regression tasks. We will monitor the training and validation loss during training.\n",
    "\n",
    "By the end of this notebook, we should have a trained model ready for evaluation and optimization in our final notebook.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data Splitting"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create Spark session\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Read the data from the CSV file\n",
    "df = spark.read.csv('processed_housing.csv', inferSchema=True, header=True)\n",
    "\n",
    "# Convert Spark DataFrame to Pandas DataFrame\n",
    "df_pandas = df.toPandas()\n",
    "\n",
    "# Perform one-hot encoding\n",
    "df_encoded = pd.get_dummies(df_pandas)\n",
    "\n",
    "# Split the data into train and test sets\n",
    "train_data, test_data = train_test_split(df_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "# Separate the features from the target variable\n",
    "X_train = train_data.drop('median_house_value', axis=1)\n",
    "y_train = train_data['median_house_value']\n",
    "\n",
    "X_test = test_data.drop('median_house_value', axis=1)\n",
    "y_test = test_data['median_house_value']\n",
    "\n",
    "# Convert data to float32\n",
    "X_train = np.array(X_train).astype('float32')\n",
    "y_train = np.array(y_train).astype('float32')\n",
    "X_test = np.array(X_test).astype('float32')\n",
    "y_test = np.array(y_test).astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "After loading the processed dataset, we convert the Spark DataFrame into a Pandas DataFrame to facilitate the further operations.\n",
    "\n",
    "Our dataset includes a categorical variable 'ocean_proximity'. Machine learning models typically require inputs to be in numerical format. Hence, we need to convert this categorical data into a numerical form. For this, we use a technique known as One-Hot Encoding. One-Hot Encoding is a process of converting categorical data variables so they can be provided to machine learning algorithms to improve predictions. With one-hot, we convert each categorical value into a new categorical column and assign a binary value of 1 or 0. Each integer value is represented as a binary vector.\n",
    "\n",
    "Then, we split our dataset into training data and testing data. Training data (80% of the dataset) is used to train our machine learning model, while testing data (20% of the dataset) is used to evaluate the model's performance.\n",
    "\n",
    "In the end, we separate the features (independent variables) from the target variable (median_house_value) and convert the data into a floating-point format, which is the preferred format for neural network models.\n",
    "\n",
    "In the next step, we will normalize our features."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# Normalizing the features\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Normalization (or scaling) is an important step in many machine learning algorithms. The goal of normalization is to change the values of numeric columns in the dataset to use a common scale, without distorting differences in the ranges of values or losing information. Normalization is also required for some algorithms to model the data correctly.\n",
    "\n",
    "Next, we will build our machine learning model using TensorFlow."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Baseline Model\n",
    "To have a point of reference for evaluating the performance of our deep learning models, we'll first build a simple linear regression model."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression MSE:  2643793400.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Create a linear regression model\n",
    "lin_reg = LinearRegression()\n",
    "\n",
    "# Train the model\n",
    "lin_reg.fit(X_train, y_train)\n",
    "\n",
    "# Get predictions on the training set\n",
    "lin_reg_preds = lin_reg.predict(X_train)\n",
    "\n",
    "# Compute the mean squared error of the predictions\n",
    "lin_reg_mse = mean_squared_error(y_train, lin_reg_preds)\n",
    "\n",
    "print(\"Linear Regression MSE: \", lin_reg_mse)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Deep Learning Model\n",
    "\n",
    "Now that we've preprocessed our data, we're ready to start building our machine learning model. We'll use a sequential model from the TensorFlow library, which is appropriate for a plain stack of layers where each layer has exactly one input tensor and one output tensor. For this task, we'll be using fully connected (dense) layers.\n",
    "\n",
    "In the following code block, we'll build and compile our model:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Define the model\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Dense(64, activation='relu', input_shape=[len(X_train[0])]),\n",
    "  tf.keras.layers.Dense(64, activation='relu'),\n",
    "  tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='mean_squared_error',\n",
    "              optimizer=tf.keras.optimizers.Adam(0.001),\n",
    "              metrics=['mae', 'mse'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Our model consists of three layers:\n",
    "\n",
    "The first layer is a dense layer with 64 nodes (or neurons), and we use the ReLU (Rectified Linear Unit) activation function.\n",
    "The second layer is also a dense layer with 64 neurons, also with the ReLU activation function.\n",
    "The third layer is the output layer, and it has just one node as we're predicting a single value (the median house value).\n",
    "After defining the model's architecture, we compile the model. During the model compilation, we specify a loss function and an optimizer, and the metrics we want to observe. Here, we are using the mean squared error as our loss function, which is a common choice for regression problems. We're using the Adam optimizer. The metrics we're monitoring are mean absolute error (MAE) and mean squared error (MSE).\n",
    "\n",
    "Next, we'll train our model."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 39028400128.0000 - mae: 178414.1250 - mse: 39028400128.0000 - val_loss: 37348364288.0000 - val_mae: 173731.6250 - val_mse: 37348364288.0000\n",
      "Epoch 2/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 36798603264.0000 - mae: 172893.6250 - mse: 36798603264.0000 - val_loss: 33002487808.0000 - val_mae: 162612.8906 - val_mse: 33002487808.0000\n",
      "Epoch 3/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 29758896128.0000 - mae: 154143.3125 - mse: 29758896128.0000 - val_loss: 23725873152.0000 - val_mae: 135621.0469 - val_mse: 23725873152.0000\n",
      "Epoch 4/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 19333296128.0000 - mae: 120438.5547 - mse: 19333296128.0000 - val_loss: 13471266816.0000 - val_mae: 96474.6953 - val_mse: 13471266816.0000\n",
      "Epoch 5/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 10432509952.0000 - mae: 82638.8984 - mse: 10432509952.0000 - val_loss: 7118215680.0000 - val_mae: 64837.1094 - val_mse: 7118215680.0000\n",
      "Epoch 6/100\n",
      "315/315 [==============================] - 0s 2ms/step - loss: 6102798336.0000 - mae: 60178.0508 - mse: 6102798336.0000 - val_loss: 5006906880.0000 - val_mae: 53990.9062 - val_mse: 5006906880.0000\n",
      "Epoch 7/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 4768290816.0000 - mae: 53364.7148 - mse: 4768290816.0000 - val_loss: 4454147584.0000 - val_mae: 51307.1523 - val_mse: 4454147584.0000\n",
      "Epoch 8/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 4308591616.0000 - mae: 50815.6055 - mse: 4308591616.0000 - val_loss: 4192077056.0000 - val_mae: 49686.9414 - val_mse: 4192077056.0000\n",
      "Epoch 9/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 4043995648.0000 - mae: 49041.6172 - mse: 4043995648.0000 - val_loss: 3990475776.0000 - val_mae: 48252.7031 - val_mse: 3990475776.0000\n",
      "Epoch 10/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 3845444352.0000 - mae: 47638.8516 - mse: 3845444352.0000 - val_loss: 3826294272.0000 - val_mae: 47055.8672 - val_mse: 3826294272.0000\n",
      "Epoch 11/100\n",
      "315/315 [==============================] - 0s 2ms/step - loss: 3683312640.0000 - mae: 46437.7930 - mse: 3683312640.0000 - val_loss: 3689000704.0000 - val_mae: 46053.7266 - val_mse: 3689000704.0000\n",
      "Epoch 12/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 3546230016.0000 - mae: 45444.7344 - mse: 3546230016.0000 - val_loss: 3571014656.0000 - val_mae: 45192.7422 - val_mse: 3571014656.0000\n",
      "Epoch 13/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 3428700416.0000 - mae: 44584.8711 - mse: 3428700416.0000 - val_loss: 3464131072.0000 - val_mae: 44352.9258 - val_mse: 3464131072.0000\n",
      "Epoch 14/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 3324805632.0000 - mae: 43774.2617 - mse: 3324805632.0000 - val_loss: 3373130240.0000 - val_mae: 43738.8242 - val_mse: 3373130240.0000\n",
      "Epoch 15/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 3232316928.0000 - mae: 43096.2617 - mse: 3232316928.0000 - val_loss: 3284050176.0000 - val_mae: 43025.6250 - val_mse: 3284050176.0000\n",
      "Epoch 16/100\n",
      "315/315 [==============================] - 0s 2ms/step - loss: 3149422592.0000 - mae: 42445.5977 - mse: 3149422592.0000 - val_loss: 3215192064.0000 - val_mae: 42525.5078 - val_mse: 3215192064.0000\n",
      "Epoch 17/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 3074121984.0000 - mae: 41902.8086 - mse: 3074121984.0000 - val_loss: 3144310784.0000 - val_mae: 41955.6328 - val_mse: 3144310784.0000\n",
      "Epoch 18/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 3006310400.0000 - mae: 41344.7500 - mse: 3006310400.0000 - val_loss: 3087874560.0000 - val_mae: 41557.7305 - val_mse: 3087874560.0000\n",
      "Epoch 19/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 2944231936.0000 - mae: 40833.8164 - mse: 2944231936.0000 - val_loss: 3032507904.0000 - val_mae: 41106.1875 - val_mse: 3032507904.0000\n",
      "Epoch 20/100\n",
      "315/315 [==============================] - 1s 3ms/step - loss: 2889196288.0000 - mae: 40392.7578 - mse: 2889196288.0000 - val_loss: 2975964672.0000 - val_mae: 40578.6250 - val_mse: 2975964672.0000\n",
      "Epoch 21/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 2839073024.0000 - mae: 39929.1328 - mse: 2839073024.0000 - val_loss: 2932301824.0000 - val_mae: 40270.2891 - val_mse: 2932301824.0000\n",
      "Epoch 22/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 2794703616.0000 - mae: 39544.0312 - mse: 2794703616.0000 - val_loss: 2896325632.0000 - val_mae: 39969.7383 - val_mse: 2896325632.0000\n",
      "Epoch 23/100\n",
      "315/315 [==============================] - 1s 3ms/step - loss: 2754844928.0000 - mae: 39198.7266 - mse: 2754844928.0000 - val_loss: 2858686720.0000 - val_mae: 39622.9727 - val_mse: 2858686720.0000\n",
      "Epoch 24/100\n",
      "315/315 [==============================] - 0s 2ms/step - loss: 2719735552.0000 - mae: 38878.1797 - mse: 2719735552.0000 - val_loss: 2823516928.0000 - val_mae: 39284.9219 - val_mse: 2823516928.0000\n",
      "Epoch 25/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 2689090560.0000 - mae: 38611.5000 - mse: 2689090560.0000 - val_loss: 2792779520.0000 - val_mae: 39027.0938 - val_mse: 2792779520.0000\n",
      "Epoch 26/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 2661568000.0000 - mae: 38302.3867 - mse: 2661568000.0000 - val_loss: 2777135872.0000 - val_mae: 38934.5977 - val_mse: 2777135872.0000\n",
      "Epoch 27/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 2636077312.0000 - mae: 38110.5547 - mse: 2636077312.0000 - val_loss: 2751351552.0000 - val_mae: 38645.1289 - val_mse: 2751351552.0000\n",
      "Epoch 28/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 2613929984.0000 - mae: 37900.9492 - mse: 2613929984.0000 - val_loss: 2731725824.0000 - val_mae: 38531.1250 - val_mse: 2731725824.0000\n",
      "Epoch 29/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 2594789632.0000 - mae: 37739.0820 - mse: 2594789632.0000 - val_loss: 2714635008.0000 - val_mae: 38353.5859 - val_mse: 2714635008.0000\n",
      "Epoch 30/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 2577058048.0000 - mae: 37563.1953 - mse: 2577058048.0000 - val_loss: 2701745408.0000 - val_mae: 38285.7539 - val_mse: 2701745408.0000\n",
      "Epoch 31/100\n",
      "315/315 [==============================] - 0s 2ms/step - loss: 2562138624.0000 - mae: 37467.9531 - mse: 2562138624.0000 - val_loss: 2684494848.0000 - val_mae: 38115.5898 - val_mse: 2684494848.0000\n",
      "Epoch 32/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 2547069952.0000 - mae: 37317.6055 - mse: 2547069952.0000 - val_loss: 2675144192.0000 - val_mae: 38170.1328 - val_mse: 2675144192.0000\n",
      "Epoch 33/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 2533673984.0000 - mae: 37249.0391 - mse: 2533673984.0000 - val_loss: 2660451072.0000 - val_mae: 37913.1094 - val_mse: 2660451072.0000\n",
      "Epoch 34/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 2523134976.0000 - mae: 37112.9219 - mse: 2523134976.0000 - val_loss: 2653083392.0000 - val_mae: 37985.2266 - val_mse: 2653083392.0000\n",
      "Epoch 35/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 2511576064.0000 - mae: 37064.0078 - mse: 2511576064.0000 - val_loss: 2642586880.0000 - val_mae: 37935.2695 - val_mse: 2642586880.0000\n",
      "Epoch 36/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 2501765120.0000 - mae: 37017.6680 - mse: 2501765120.0000 - val_loss: 2632970496.0000 - val_mae: 37817.2305 - val_mse: 2632970496.0000\n",
      "Epoch 37/100\n",
      "315/315 [==============================] - 0s 2ms/step - loss: 2493622016.0000 - mae: 36897.4844 - mse: 2493622016.0000 - val_loss: 2631488512.0000 - val_mae: 37951.7266 - val_mse: 2631488512.0000\n",
      "Epoch 38/100\n",
      "315/315 [==============================] - 0s 1ms/step - loss: 2484843008.0000 - mae: 36873.5898 - mse: 2484843008.0000 - val_loss: 2620087552.0000 - val_mae: 37738.6719 - val_mse: 2620087552.0000\n",
      "Epoch 39/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 2477308160.0000 - mae: 36804.6641 - mse: 2477308160.0000 - val_loss: 2614107136.0000 - val_mae: 37693.7188 - val_mse: 2614107136.0000\n",
      "Epoch 40/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 2469277696.0000 - mae: 36750.8555 - mse: 2469277696.0000 - val_loss: 2607759104.0000 - val_mae: 37677.4258 - val_mse: 2607759104.0000\n",
      "Epoch 41/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 2463625216.0000 - mae: 36703.7969 - mse: 2463625216.0000 - val_loss: 2600622592.0000 - val_mae: 37667.2070 - val_mse: 2600622592.0000\n",
      "Epoch 42/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 2456877312.0000 - mae: 36670.0352 - mse: 2456877312.0000 - val_loss: 2593825536.0000 - val_mae: 37599.9609 - val_mse: 2593825536.0000\n",
      "Epoch 43/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 2450426368.0000 - mae: 36629.8516 - mse: 2450426368.0000 - val_loss: 2588977920.0000 - val_mae: 37555.1719 - val_mse: 2588977920.0000\n",
      "Epoch 44/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 2444979456.0000 - mae: 36566.2656 - mse: 2444979456.0000 - val_loss: 2585348352.0000 - val_mae: 37557.1211 - val_mse: 2585348352.0000\n",
      "Epoch 45/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 2439698432.0000 - mae: 36512.7852 - mse: 2439698432.0000 - val_loss: 2590724096.0000 - val_mae: 37678.8672 - val_mse: 2590724096.0000\n",
      "Epoch 46/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 2434773248.0000 - mae: 36495.0352 - mse: 2434773248.0000 - val_loss: 2579247616.0000 - val_mae: 37554.0391 - val_mse: 2579247616.0000\n",
      "Epoch 47/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 2428929536.0000 - mae: 36463.9609 - mse: 2428929536.0000 - val_loss: 2576853248.0000 - val_mae: 37536.1289 - val_mse: 2576853248.0000\n",
      "Epoch 48/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 2424993024.0000 - mae: 36438.6016 - mse: 2424993024.0000 - val_loss: 2571491072.0000 - val_mae: 37477.1211 - val_mse: 2571491072.0000\n",
      "Epoch 49/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 2420620288.0000 - mae: 36376.6250 - mse: 2420620288.0000 - val_loss: 2569223168.0000 - val_mae: 37469.7812 - val_mse: 2569223168.0000\n",
      "Epoch 50/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 2416832512.0000 - mae: 36371.8789 - mse: 2416832512.0000 - val_loss: 2567026432.0000 - val_mae: 37454.5234 - val_mse: 2567026432.0000\n",
      "Epoch 51/100\n",
      "315/315 [==============================] - 0s 2ms/step - loss: 2412310784.0000 - mae: 36313.6211 - mse: 2412310784.0000 - val_loss: 2561507328.0000 - val_mae: 37482.7031 - val_mse: 2561507328.0000\n",
      "Epoch 52/100\n",
      "315/315 [==============================] - 0s 2ms/step - loss: 2408324608.0000 - mae: 36346.2891 - mse: 2408324608.0000 - val_loss: 2558724352.0000 - val_mae: 37397.8047 - val_mse: 2558724352.0000\n",
      "Epoch 53/100\n",
      "315/315 [==============================] - 0s 2ms/step - loss: 2404157952.0000 - mae: 36233.9961 - mse: 2404157952.0000 - val_loss: 2559582976.0000 - val_mae: 37516.0078 - val_mse: 2559582976.0000\n",
      "Epoch 54/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 2400411136.0000 - mae: 36280.9180 - mse: 2400411136.0000 - val_loss: 2552008192.0000 - val_mae: 37402.3789 - val_mse: 2552008192.0000\n",
      "Epoch 55/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 2395979008.0000 - mae: 36195.3555 - mse: 2395979008.0000 - val_loss: 2550746624.0000 - val_mae: 37353.5977 - val_mse: 2550746624.0000\n",
      "Epoch 56/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 2393032192.0000 - mae: 36170.4102 - mse: 2393032192.0000 - val_loss: 2550470656.0000 - val_mae: 37415.0938 - val_mse: 2550470656.0000\n",
      "Epoch 57/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 2390123264.0000 - mae: 36169.0625 - mse: 2390123264.0000 - val_loss: 2545083648.0000 - val_mae: 37355.0781 - val_mse: 2545083648.0000\n",
      "Epoch 58/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 2386607616.0000 - mae: 36116.5312 - mse: 2386607616.0000 - val_loss: 2545739008.0000 - val_mae: 37396.5391 - val_mse: 2545739008.0000\n",
      "Epoch 59/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 2384071424.0000 - mae: 36165.7578 - mse: 2384071424.0000 - val_loss: 2539167744.0000 - val_mae: 37263.2617 - val_mse: 2539167744.0000\n",
      "Epoch 60/100\n",
      "315/315 [==============================] - 0s 2ms/step - loss: 2380298496.0000 - mae: 36054.6875 - mse: 2380298496.0000 - val_loss: 2541242368.0000 - val_mae: 37368.0430 - val_mse: 2541242368.0000\n",
      "Epoch 61/100\n",
      "315/315 [==============================] - 0s 2ms/step - loss: 2378259968.0000 - mae: 36121.3789 - mse: 2378259968.0000 - val_loss: 2537512960.0000 - val_mae: 37301.8086 - val_mse: 2537512960.0000\n",
      "Epoch 62/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 2375458048.0000 - mae: 36041.4805 - mse: 2375458048.0000 - val_loss: 2530587904.0000 - val_mae: 37232.2461 - val_mse: 2530587904.0000\n",
      "Epoch 63/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 2371756032.0000 - mae: 36033.0586 - mse: 2371756032.0000 - val_loss: 2529816064.0000 - val_mae: 37183.4922 - val_mse: 2529816064.0000\n",
      "Epoch 64/100\n",
      "315/315 [==============================] - 0s 2ms/step - loss: 2369100032.0000 - mae: 36017.2539 - mse: 2369100032.0000 - val_loss: 2521231616.0000 - val_mae: 37068.8125 - val_mse: 2521231616.0000\n",
      "Epoch 65/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 2367122688.0000 - mae: 35969.4453 - mse: 2367122688.0000 - val_loss: 2522010624.0000 - val_mae: 37097.7539 - val_mse: 2522010624.0000\n",
      "Epoch 66/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 2363949824.0000 - mae: 35936.9727 - mse: 2363949824.0000 - val_loss: 2525086976.0000 - val_mae: 37198.9648 - val_mse: 2525086976.0000\n",
      "Epoch 67/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 2361346304.0000 - mae: 35969.8633 - mse: 2361346304.0000 - val_loss: 2517817088.0000 - val_mae: 37077.7344 - val_mse: 2517817088.0000\n",
      "Epoch 68/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 2359313920.0000 - mae: 35903.6992 - mse: 2359313920.0000 - val_loss: 2516363520.0000 - val_mae: 37066.7031 - val_mse: 2516363520.0000\n",
      "Epoch 69/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 2356924928.0000 - mae: 35909.5156 - mse: 2356924928.0000 - val_loss: 2512852992.0000 - val_mae: 37012.1758 - val_mse: 2512852992.0000\n",
      "Epoch 70/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 2354579968.0000 - mae: 35860.8828 - mse: 2354579968.0000 - val_loss: 2518073088.0000 - val_mae: 37141.7695 - val_mse: 2518073088.0000\n",
      "Epoch 71/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 2352306176.0000 - mae: 35902.0430 - mse: 2352306176.0000 - val_loss: 2509524736.0000 - val_mae: 36979.7422 - val_mse: 2509524736.0000\n",
      "Epoch 72/100\n",
      "315/315 [==============================] - 0s 2ms/step - loss: 2349555968.0000 - mae: 35819.7031 - mse: 2349555968.0000 - val_loss: 2512927488.0000 - val_mae: 37106.2422 - val_mse: 2512927488.0000\n",
      "Epoch 73/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 2348111872.0000 - mae: 35871.2969 - mse: 2348111872.0000 - val_loss: 2509111552.0000 - val_mae: 37002.7383 - val_mse: 2509111552.0000\n",
      "Epoch 74/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 2346191872.0000 - mae: 35804.2266 - mse: 2346191872.0000 - val_loss: 2507091456.0000 - val_mae: 36981.6328 - val_mse: 2507091456.0000\n",
      "Epoch 75/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 2342541568.0000 - mae: 35740.8359 - mse: 2342541568.0000 - val_loss: 2515214336.0000 - val_mae: 37219.8594 - val_mse: 2515214336.0000\n",
      "Epoch 76/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 2341924864.0000 - mae: 35809.2109 - mse: 2341924864.0000 - val_loss: 2503942656.0000 - val_mae: 36974.8086 - val_mse: 2503942656.0000\n",
      "Epoch 77/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 2338821376.0000 - mae: 35790.8789 - mse: 2338821376.0000 - val_loss: 2496593408.0000 - val_mae: 36784.6523 - val_mse: 2496593408.0000\n",
      "Epoch 78/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 2337983232.0000 - mae: 35724.7617 - mse: 2337983232.0000 - val_loss: 2496361472.0000 - val_mae: 36883.0117 - val_mse: 2496361472.0000\n",
      "Epoch 79/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 2335300352.0000 - mae: 35717.4961 - mse: 2335300352.0000 - val_loss: 2501494528.0000 - val_mae: 36969.2109 - val_mse: 2501494528.0000\n",
      "Epoch 80/100\n",
      "315/315 [==============================] - 0s 1ms/step - loss: 2333801472.0000 - mae: 35733.3125 - mse: 2333801472.0000 - val_loss: 2489191936.0000 - val_mae: 36710.8750 - val_mse: 2489191936.0000\n",
      "Epoch 81/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 2331841280.0000 - mae: 35677.2266 - mse: 2331841280.0000 - val_loss: 2493911040.0000 - val_mae: 36889.0938 - val_mse: 2493911040.0000\n",
      "Epoch 82/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 2330434560.0000 - mae: 35652.8086 - mse: 2330434560.0000 - val_loss: 2500582144.0000 - val_mae: 37007.3633 - val_mse: 2500582144.0000\n",
      "Epoch 83/100\n",
      "315/315 [==============================] - 0s 2ms/step - loss: 2329058560.0000 - mae: 35702.9531 - mse: 2329058560.0000 - val_loss: 2493303296.0000 - val_mae: 36845.6992 - val_mse: 2493303296.0000\n",
      "Epoch 84/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 2326366976.0000 - mae: 35668.6758 - mse: 2326366976.0000 - val_loss: 2481670144.0000 - val_mae: 36638.8086 - val_mse: 2481670144.0000\n",
      "Epoch 85/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 2325438464.0000 - mae: 35612.7188 - mse: 2325438464.0000 - val_loss: 2488504064.0000 - val_mae: 36771.3906 - val_mse: 2488504064.0000\n",
      "Epoch 86/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 2323542016.0000 - mae: 35660.5234 - mse: 2323542016.0000 - val_loss: 2482295296.0000 - val_mae: 36608.3242 - val_mse: 2482295296.0000\n",
      "Epoch 87/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 2321927936.0000 - mae: 35581.8320 - mse: 2321927936.0000 - val_loss: 2490328320.0000 - val_mae: 36874.8242 - val_mse: 2490328320.0000\n",
      "Epoch 88/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 2320628992.0000 - mae: 35599.5469 - mse: 2320628992.0000 - val_loss: 2484078336.0000 - val_mae: 36790.1367 - val_mse: 2484078336.0000\n",
      "Epoch 89/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 2319230464.0000 - mae: 35614.0352 - mse: 2319230464.0000 - val_loss: 2481692416.0000 - val_mae: 36732.7500 - val_mse: 2481692416.0000\n",
      "Epoch 90/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 2316775424.0000 - mae: 35558.6406 - mse: 2316775424.0000 - val_loss: 2480332544.0000 - val_mae: 36646.6992 - val_mse: 2480332544.0000\n",
      "Epoch 91/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 2315732736.0000 - mae: 35566.6445 - mse: 2315732736.0000 - val_loss: 2477169152.0000 - val_mae: 36598.3516 - val_mse: 2477169152.0000\n",
      "Epoch 92/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 2314331392.0000 - mae: 35522.2969 - mse: 2314331392.0000 - val_loss: 2478966016.0000 - val_mae: 36689.2500 - val_mse: 2478966016.0000\n",
      "Epoch 93/100\n",
      "315/315 [==============================] - 0s 2ms/step - loss: 2313428480.0000 - mae: 35536.9805 - mse: 2313428480.0000 - val_loss: 2478886912.0000 - val_mae: 36713.7383 - val_mse: 2478886912.0000\n",
      "Epoch 94/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 2311622400.0000 - mae: 35540.2773 - mse: 2311622400.0000 - val_loss: 2479194112.0000 - val_mae: 36676.1758 - val_mse: 2479194112.0000\n",
      "Epoch 95/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 2309536768.0000 - mae: 35516.1484 - mse: 2309536768.0000 - val_loss: 2472796672.0000 - val_mae: 36535.6719 - val_mse: 2472796672.0000\n",
      "Epoch 96/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 2308050432.0000 - mae: 35429.0820 - mse: 2308050432.0000 - val_loss: 2481546752.0000 - val_mae: 36805.6406 - val_mse: 2481546752.0000\n",
      "Epoch 97/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 2307714048.0000 - mae: 35486.9727 - mse: 2307714048.0000 - val_loss: 2480669952.0000 - val_mae: 36798.5000 - val_mse: 2480669952.0000\n",
      "Epoch 98/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 2306346496.0000 - mae: 35489.3477 - mse: 2306346496.0000 - val_loss: 2474022400.0000 - val_mae: 36670.6484 - val_mse: 2474022400.0000\n",
      "Epoch 99/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 2304684032.0000 - mae: 35472.0898 - mse: 2304684032.0000 - val_loss: 2472329472.0000 - val_mae: 36668.9922 - val_mse: 2472329472.0000\n",
      "Epoch 100/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 2304469760.0000 - mae: 35468.5586 - mse: 2304469760.0000 - val_loss: 2468518656.0000 - val_mae: 36599.4727 - val_mse: 2468518656.0000\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=100, validation_split = 0.2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here, we train the model for 100 epochs with a validation split of 0.2, meaning that 20% of the training data is used as validation data.\n",
    "\n",
    "In the next step, we'll evaluate our model's performance."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model Improvement\n",
    "To improve our model's performance, we can adjust its architecture and hyperparameters. One approach might be to add more layers, use different types of layers, such as dropout for regularization, or tweak the learning rate."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "315/315 [==============================] - 2s 2ms/step - loss: 39067320320.0000 - mae: 178514.2969 - mse: 39067320320.0000 - val_loss: 37503205376.0000 - val_mae: 174133.5781 - val_mse: 37503205376.0000\n",
      "Epoch 2/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 37368135680.0000 - mae: 174357.5469 - mse: 37368135680.0000 - val_loss: 34095173632.0000 - val_mae: 165561.4688 - val_mse: 34095173632.0000\n",
      "Epoch 3/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 31718864896.0000 - mae: 159691.6094 - mse: 31718864896.0000 - val_loss: 26393835520.0000 - val_mae: 144124.9844 - val_mse: 26393835520.0000\n",
      "Epoch 4/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 22656012288.0000 - mae: 132215.4219 - mse: 22656012288.0000 - val_loss: 16759936000.0000 - val_mae: 110756.3828 - val_mse: 16759936000.0000\n",
      "Epoch 5/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 13647782912.0000 - mae: 97749.8984 - mse: 13647782912.0000 - val_loss: 9436935168.0000 - val_mae: 77997.6094 - val_mse: 9436935168.0000\n",
      "Epoch 6/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 8221757952.0000 - mae: 71980.5469 - mse: 8221757952.0000 - val_loss: 6048324608.0000 - val_mae: 60057.2109 - val_mse: 6048324608.0000\n",
      "Epoch 7/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 6001297920.0000 - mae: 60300.7266 - mse: 6001297920.0000 - val_loss: 4932345344.0000 - val_mae: 53809.9062 - val_mse: 4932345344.0000\n",
      "Epoch 8/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 5266742272.0000 - mae: 56170.8477 - mse: 5266742272.0000 - val_loss: 4523501056.0000 - val_mae: 51425.2539 - val_mse: 4523501056.0000\n",
      "Epoch 9/100\n",
      "315/315 [==============================] - 1s 3ms/step - loss: 4869739520.0000 - mae: 53840.9844 - mse: 4869739520.0000 - val_loss: 4273516032.0000 - val_mae: 49882.8711 - val_mse: 4273516032.0000\n",
      "Epoch 10/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 4633274880.0000 - mae: 52408.5469 - mse: 4633274880.0000 - val_loss: 4072624640.0000 - val_mae: 48487.0781 - val_mse: 4072624640.0000\n",
      "Epoch 11/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 4418212352.0000 - mae: 51130.2500 - mse: 4418212352.0000 - val_loss: 3906448640.0000 - val_mae: 47336.9062 - val_mse: 3906448640.0000\n",
      "Epoch 12/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 4309438976.0000 - mae: 50171.6406 - mse: 4309438976.0000 - val_loss: 3762692352.0000 - val_mae: 46312.0820 - val_mse: 3762692352.0000\n",
      "Epoch 13/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 4118014720.0000 - mae: 49036.6094 - mse: 4118014720.0000 - val_loss: 3639351296.0000 - val_mae: 45368.6016 - val_mse: 3639351296.0000\n",
      "Epoch 14/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 4019612672.0000 - mae: 48159.1758 - mse: 4019612672.0000 - val_loss: 3527021056.0000 - val_mae: 44512.0469 - val_mse: 3527021056.0000\n",
      "Epoch 15/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 3919249408.0000 - mae: 47593.8125 - mse: 3919249408.0000 - val_loss: 3424548608.0000 - val_mae: 43776.1836 - val_mse: 3424548608.0000\n",
      "Epoch 16/100\n",
      "315/315 [==============================] - 1s 3ms/step - loss: 3825692160.0000 - mae: 46923.1992 - mse: 3825692160.0000 - val_loss: 3338971392.0000 - val_mae: 43141.6523 - val_mse: 3338971392.0000\n",
      "Epoch 17/100\n",
      "315/315 [==============================] - 1s 3ms/step - loss: 3689729024.0000 - mae: 46120.7891 - mse: 3689729024.0000 - val_loss: 3256426496.0000 - val_mae: 42492.0156 - val_mse: 3256426496.0000\n",
      "Epoch 18/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 3637819904.0000 - mae: 45802.9219 - mse: 3637819904.0000 - val_loss: 3185689600.0000 - val_mae: 41977.3828 - val_mse: 3185689600.0000\n",
      "Epoch 19/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 3538161920.0000 - mae: 45102.5703 - mse: 3538161920.0000 - val_loss: 3115994112.0000 - val_mae: 41378.8281 - val_mse: 3115994112.0000\n",
      "Epoch 20/100\n",
      "315/315 [==============================] - 1s 3ms/step - loss: 3480143104.0000 - mae: 44458.9492 - mse: 3480143104.0000 - val_loss: 3055521536.0000 - val_mae: 40930.2188 - val_mse: 3055521536.0000\n",
      "Epoch 21/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 3414631168.0000 - mae: 44013.0000 - mse: 3414631168.0000 - val_loss: 3003159296.0000 - val_mae: 40609.0273 - val_mse: 3003159296.0000\n",
      "Epoch 22/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 3355420416.0000 - mae: 43657.6250 - mse: 3355420416.0000 - val_loss: 2954713088.0000 - val_mae: 40152.3633 - val_mse: 2954713088.0000\n",
      "Epoch 23/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 3354280704.0000 - mae: 43620.5391 - mse: 3354280704.0000 - val_loss: 2908686080.0000 - val_mae: 39780.1680 - val_mse: 2908686080.0000\n",
      "Epoch 24/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 3260152064.0000 - mae: 42760.5195 - mse: 3260152064.0000 - val_loss: 2872374528.0000 - val_mae: 39513.5352 - val_mse: 2872374528.0000\n",
      "Epoch 25/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 3238254336.0000 - mae: 42568.2266 - mse: 3238254336.0000 - val_loss: 2840597248.0000 - val_mae: 39261.6094 - val_mse: 2840597248.0000\n",
      "Epoch 26/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 3209443840.0000 - mae: 42236.6914 - mse: 3209443840.0000 - val_loss: 2809867008.0000 - val_mae: 38964.7930 - val_mse: 2809867008.0000\n",
      "Epoch 27/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 3199052032.0000 - mae: 42258.5508 - mse: 3199052032.0000 - val_loss: 2785525760.0000 - val_mae: 38698.9570 - val_mse: 2785525760.0000\n",
      "Epoch 28/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 3215461888.0000 - mae: 42180.1133 - mse: 3215461888.0000 - val_loss: 2758670592.0000 - val_mae: 38554.3984 - val_mse: 2758670592.0000\n",
      "Epoch 29/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 3135806720.0000 - mae: 41858.8750 - mse: 3135806720.0000 - val_loss: 2738272000.0000 - val_mae: 38278.3477 - val_mse: 2738272000.0000\n",
      "Epoch 30/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 3109410560.0000 - mae: 41573.5703 - mse: 3109410560.0000 - val_loss: 2721494272.0000 - val_mae: 38216.0938 - val_mse: 2721494272.0000\n",
      "Epoch 31/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 3055517696.0000 - mae: 41275.5312 - mse: 3055517696.0000 - val_loss: 2705763840.0000 - val_mae: 38118.4102 - val_mse: 2705763840.0000\n",
      "Epoch 32/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 3062471424.0000 - mae: 41216.5820 - mse: 3062471424.0000 - val_loss: 2691400448.0000 - val_mae: 37986.6992 - val_mse: 2691400448.0000\n",
      "Epoch 33/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 3066087936.0000 - mae: 41207.9844 - mse: 3066087936.0000 - val_loss: 2678461696.0000 - val_mae: 37921.2109 - val_mse: 2678461696.0000\n",
      "Epoch 34/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 3013096448.0000 - mae: 40916.3398 - mse: 3013096448.0000 - val_loss: 2665921280.0000 - val_mae: 37799.5352 - val_mse: 2665921280.0000\n",
      "Epoch 35/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 3046464256.0000 - mae: 41022.3359 - mse: 3046464256.0000 - val_loss: 2655129856.0000 - val_mae: 37714.8906 - val_mse: 2655129856.0000\n",
      "Epoch 36/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 2990771712.0000 - mae: 40685.0430 - mse: 2990771712.0000 - val_loss: 2647340032.0000 - val_mae: 37580.3906 - val_mse: 2647340032.0000\n",
      "Epoch 37/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 3011588352.0000 - mae: 40767.1094 - mse: 3011588352.0000 - val_loss: 2636949504.0000 - val_mae: 37598.2031 - val_mse: 2636949504.0000\n",
      "Epoch 38/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 3004918528.0000 - mae: 40761.2852 - mse: 3004918528.0000 - val_loss: 2630625280.0000 - val_mae: 37615.4531 - val_mse: 2630625280.0000\n",
      "Epoch 39/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 2986850304.0000 - mae: 40595.6367 - mse: 2986850304.0000 - val_loss: 2622598656.0000 - val_mae: 37490.2812 - val_mse: 2622598656.0000\n",
      "Epoch 40/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 2966776064.0000 - mae: 40470.2578 - mse: 2966776064.0000 - val_loss: 2615936768.0000 - val_mae: 37472.7500 - val_mse: 2615936768.0000\n",
      "Epoch 41/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 3004918272.0000 - mae: 40650.9883 - mse: 3004918272.0000 - val_loss: 2612476672.0000 - val_mae: 37410.2109 - val_mse: 2612476672.0000\n",
      "Epoch 42/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 2967042816.0000 - mae: 40407.9883 - mse: 2967042816.0000 - val_loss: 2606992640.0000 - val_mae: 37397.0859 - val_mse: 2606992640.0000\n",
      "Epoch 43/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 2960186112.0000 - mae: 40403.5430 - mse: 2960186112.0000 - val_loss: 2597900544.0000 - val_mae: 37363.6055 - val_mse: 2597900544.0000\n",
      "Epoch 44/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 2981670656.0000 - mae: 40508.7969 - mse: 2981670656.0000 - val_loss: 2593805568.0000 - val_mae: 37395.4141 - val_mse: 2593805568.0000\n",
      "Epoch 45/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 2914036480.0000 - mae: 40067.3828 - mse: 2914036480.0000 - val_loss: 2593221888.0000 - val_mae: 37354.1250 - val_mse: 2593221888.0000\n",
      "Epoch 46/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 2900742912.0000 - mae: 40027.0859 - mse: 2900742912.0000 - val_loss: 2585936640.0000 - val_mae: 37272.2344 - val_mse: 2585936640.0000\n",
      "Epoch 47/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 2928809472.0000 - mae: 40352.0781 - mse: 2928809472.0000 - val_loss: 2584882432.0000 - val_mae: 37200.2812 - val_mse: 2584882432.0000\n",
      "Epoch 48/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 2974707456.0000 - mae: 40500.2266 - mse: 2974707456.0000 - val_loss: 2578706176.0000 - val_mae: 37173.0781 - val_mse: 2578706176.0000\n",
      "Epoch 49/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 2938389248.0000 - mae: 40155.0586 - mse: 2938389248.0000 - val_loss: 2573276672.0000 - val_mae: 37223.4180 - val_mse: 2573276672.0000\n",
      "Epoch 50/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 2920892416.0000 - mae: 40237.2383 - mse: 2920892416.0000 - val_loss: 2569302272.0000 - val_mae: 37214.7773 - val_mse: 2569302272.0000\n",
      "Epoch 51/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 2928893440.0000 - mae: 40244.2305 - mse: 2928893440.0000 - val_loss: 2569859072.0000 - val_mae: 37169.6328 - val_mse: 2569859072.0000\n",
      "Epoch 52/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 2910898944.0000 - mae: 40116.7773 - mse: 2910898944.0000 - val_loss: 2568890624.0000 - val_mae: 37233.8828 - val_mse: 2568890624.0000\n",
      "Epoch 53/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 2895200512.0000 - mae: 39914.2344 - mse: 2895200512.0000 - val_loss: 2561609728.0000 - val_mae: 37177.4492 - val_mse: 2561609728.0000\n",
      "Epoch 54/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 2851260416.0000 - mae: 39671.7852 - mse: 2851260416.0000 - val_loss: 2557520128.0000 - val_mae: 37093.9180 - val_mse: 2557520128.0000\n",
      "Epoch 55/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 2898858752.0000 - mae: 39990.8672 - mse: 2898858752.0000 - val_loss: 2556132352.0000 - val_mae: 37109.0156 - val_mse: 2556132352.0000\n",
      "Epoch 56/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 2864777984.0000 - mae: 39770.7539 - mse: 2864777984.0000 - val_loss: 2554578944.0000 - val_mae: 37081.1445 - val_mse: 2554578944.0000\n",
      "Epoch 57/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 2914532096.0000 - mae: 39957.4375 - mse: 2914532096.0000 - val_loss: 2551723776.0000 - val_mae: 37126.6680 - val_mse: 2551723776.0000\n",
      "Epoch 58/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 2866961408.0000 - mae: 39749.3711 - mse: 2866961408.0000 - val_loss: 2549461504.0000 - val_mae: 37079.6719 - val_mse: 2549461504.0000\n",
      "Epoch 59/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 2863863296.0000 - mae: 39770.0234 - mse: 2863863296.0000 - val_loss: 2546337536.0000 - val_mae: 37049.0117 - val_mse: 2546337536.0000\n",
      "Epoch 60/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 2900776448.0000 - mae: 40132.7578 - mse: 2900776448.0000 - val_loss: 2544506880.0000 - val_mae: 37048.5898 - val_mse: 2544506880.0000\n",
      "Epoch 61/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 2866552320.0000 - mae: 39800.6289 - mse: 2866552320.0000 - val_loss: 2542002432.0000 - val_mae: 37098.3125 - val_mse: 2542002432.0000\n",
      "Epoch 62/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 2873651712.0000 - mae: 39921.5312 - mse: 2873651712.0000 - val_loss: 2538227456.0000 - val_mae: 36982.8008 - val_mse: 2538227456.0000\n",
      "Epoch 63/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 2849526784.0000 - mae: 39671.9805 - mse: 2849526784.0000 - val_loss: 2539652096.0000 - val_mae: 37040.3320 - val_mse: 2539652096.0000\n",
      "Epoch 64/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 2840120064.0000 - mae: 39547.5391 - mse: 2840120064.0000 - val_loss: 2537344512.0000 - val_mae: 36969.1445 - val_mse: 2537344512.0000\n",
      "Epoch 65/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 2822594816.0000 - mae: 39422.2305 - mse: 2822594816.0000 - val_loss: 2533307648.0000 - val_mae: 36938.6836 - val_mse: 2533307648.0000\n",
      "Epoch 66/100\n",
      "315/315 [==============================] - 1s 3ms/step - loss: 2843502336.0000 - mae: 39725.9805 - mse: 2843502336.0000 - val_loss: 2530338560.0000 - val_mae: 36930.1055 - val_mse: 2530338560.0000\n",
      "Epoch 67/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 2883032320.0000 - mae: 39823.9883 - mse: 2883032320.0000 - val_loss: 2529832960.0000 - val_mae: 36950.9922 - val_mse: 2529832960.0000\n",
      "Epoch 68/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 2874528000.0000 - mae: 39747.3945 - mse: 2874528000.0000 - val_loss: 2526965504.0000 - val_mae: 36901.0234 - val_mse: 2526965504.0000\n",
      "Epoch 69/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 2872857600.0000 - mae: 39697.2773 - mse: 2872857600.0000 - val_loss: 2521279488.0000 - val_mae: 36879.6016 - val_mse: 2521279488.0000\n",
      "Epoch 70/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 2868136704.0000 - mae: 39789.2930 - mse: 2868136704.0000 - val_loss: 2523379200.0000 - val_mae: 36820.7773 - val_mse: 2523379200.0000\n",
      "Epoch 71/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 2834104064.0000 - mae: 39585.0430 - mse: 2834104064.0000 - val_loss: 2521321216.0000 - val_mae: 36931.4258 - val_mse: 2521321216.0000\n",
      "Epoch 72/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 2829103104.0000 - mae: 39669.1406 - mse: 2829103104.0000 - val_loss: 2524109312.0000 - val_mae: 36975.9414 - val_mse: 2524109312.0000\n",
      "Epoch 73/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 2839367424.0000 - mae: 39477.3516 - mse: 2839367424.0000 - val_loss: 2518073856.0000 - val_mae: 36773.2031 - val_mse: 2518073856.0000\n",
      "Epoch 74/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 2826992128.0000 - mae: 39459.2031 - mse: 2826992128.0000 - val_loss: 2516867328.0000 - val_mae: 36840.4609 - val_mse: 2516867328.0000\n",
      "Epoch 75/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 2852044032.0000 - mae: 39783.5859 - mse: 2852044032.0000 - val_loss: 2518081792.0000 - val_mae: 36995.2383 - val_mse: 2518081792.0000\n",
      "Epoch 76/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 2836834560.0000 - mae: 39540.9062 - mse: 2836834560.0000 - val_loss: 2514964480.0000 - val_mae: 36855.2812 - val_mse: 2514964480.0000\n",
      "Epoch 77/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 2830674176.0000 - mae: 39636.0742 - mse: 2830674176.0000 - val_loss: 2512276224.0000 - val_mae: 36765.6484 - val_mse: 2512276224.0000\n",
      "Epoch 78/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 2812208128.0000 - mae: 39146.3945 - mse: 2812208128.0000 - val_loss: 2509407744.0000 - val_mae: 36722.2031 - val_mse: 2509407744.0000\n",
      "Epoch 79/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 2829908224.0000 - mae: 39590.8242 - mse: 2829908224.0000 - val_loss: 2511074816.0000 - val_mae: 36846.5430 - val_mse: 2511074816.0000\n",
      "Epoch 80/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 2818437376.0000 - mae: 39490.0781 - mse: 2818437376.0000 - val_loss: 2508245504.0000 - val_mae: 36656.5664 - val_mse: 2508245504.0000\n",
      "Epoch 81/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 2815588096.0000 - mae: 39428.1562 - mse: 2815588096.0000 - val_loss: 2508493312.0000 - val_mae: 36680.2461 - val_mse: 2508493312.0000\n",
      "Epoch 82/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 2814824960.0000 - mae: 39389.9805 - mse: 2814824960.0000 - val_loss: 2504491776.0000 - val_mae: 36628.4180 - val_mse: 2504491776.0000\n",
      "Epoch 83/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 2858089472.0000 - mae: 39666.6523 - mse: 2858089472.0000 - val_loss: 2505947904.0000 - val_mae: 36685.2969 - val_mse: 2505947904.0000\n",
      "Epoch 84/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 2798328320.0000 - mae: 39337.7969 - mse: 2798328320.0000 - val_loss: 2501307648.0000 - val_mae: 36682.5000 - val_mse: 2501307648.0000\n",
      "Epoch 85/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 2852213760.0000 - mae: 39507.9492 - mse: 2852213760.0000 - val_loss: 2501951232.0000 - val_mae: 36709.2617 - val_mse: 2501951232.0000\n",
      "Epoch 86/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 2843987456.0000 - mae: 39638.8516 - mse: 2843987456.0000 - val_loss: 2500207360.0000 - val_mae: 36688.2578 - val_mse: 2500207360.0000\n",
      "Epoch 87/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 2795848704.0000 - mae: 39424.0508 - mse: 2795848704.0000 - val_loss: 2502183680.0000 - val_mae: 36778.1328 - val_mse: 2502183680.0000\n",
      "Epoch 88/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 2852122624.0000 - mae: 39736.4883 - mse: 2852122624.0000 - val_loss: 2495834368.0000 - val_mae: 36702.9453 - val_mse: 2495834368.0000\n",
      "Epoch 89/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 2767101952.0000 - mae: 39034.0586 - mse: 2767101952.0000 - val_loss: 2496384256.0000 - val_mae: 36756.6875 - val_mse: 2496384256.0000\n",
      "Epoch 90/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 2820040960.0000 - mae: 39286.0234 - mse: 2820040960.0000 - val_loss: 2496896512.0000 - val_mae: 36743.1875 - val_mse: 2496896512.0000\n",
      "Epoch 91/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 2818391296.0000 - mae: 39415.7500 - mse: 2818391296.0000 - val_loss: 2493718528.0000 - val_mae: 36651.1641 - val_mse: 2493718528.0000\n",
      "Epoch 92/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 2817646080.0000 - mae: 39357.2031 - mse: 2817646080.0000 - val_loss: 2494345984.0000 - val_mae: 36621.5781 - val_mse: 2494345984.0000\n",
      "Epoch 93/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 2750425600.0000 - mae: 38940.8711 - mse: 2750425600.0000 - val_loss: 2490620672.0000 - val_mae: 36745.4805 - val_mse: 2490620672.0000\n",
      "Epoch 94/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 2743606272.0000 - mae: 39178.7773 - mse: 2743606272.0000 - val_loss: 2487163904.0000 - val_mae: 36647.0273 - val_mse: 2487163904.0000\n",
      "Epoch 95/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 2771781888.0000 - mae: 39116.6758 - mse: 2771781888.0000 - val_loss: 2487906304.0000 - val_mae: 36585.3867 - val_mse: 2487906304.0000\n",
      "Epoch 96/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 2812930816.0000 - mae: 39393.1094 - mse: 2812930816.0000 - val_loss: 2487375872.0000 - val_mae: 36656.3164 - val_mse: 2487375872.0000\n",
      "Epoch 97/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 2816646144.0000 - mae: 39429.1875 - mse: 2816646144.0000 - val_loss: 2487319808.0000 - val_mae: 36582.9453 - val_mse: 2487319808.0000\n",
      "Epoch 98/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 2773801728.0000 - mae: 39208.7734 - mse: 2773801728.0000 - val_loss: 2483334912.0000 - val_mae: 36662.1289 - val_mse: 2483334912.0000\n",
      "Epoch 99/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 2828282880.0000 - mae: 39508.9844 - mse: 2828282880.0000 - val_loss: 2484008704.0000 - val_mae: 36565.3281 - val_mse: 2484008704.0000\n",
      "Epoch 100/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 2750394112.0000 - mae: 39039.9766 - mse: 2750394112.0000 - val_loss: 2483096320.0000 - val_mae: 36558.8906 - val_mse: 2483096320.0000\n"
     ]
    }
   ],
   "source": [
    "# Redefine the model with dropout\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Dense(64, activation='relu', input_shape=[len(X_train[0])]),\n",
    "  tf.keras.layers.Dropout(0.2),\n",
    "  tf.keras.layers.Dense(64, activation='relu'),\n",
    "  tf.keras.layers.Dropout(0.2),\n",
    "  tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "# Recompile the model\n",
    "model.compile(loss='mean_squared_error',\n",
    "              optimizer=tf.keras.optimizers.Adam(0.001),\n",
    "              metrics=['mae', 'mse'])\n",
    "\n",
    "# Retrain the model\n",
    "history = model.fit(X_train, y_train, epochs=100, validation_split = 0.2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "By incorporating dropout layers, we're introducing a form of regularization. During training, dropout will randomly set a fraction of input units to 0 at each update, which helps prevent overfitting."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model Training\n",
    "We have already trained our model on the training data using the appropriate loss functions and optimizers for our regression task. We have also monitored the training and validation loss, which can provide useful insight into the performance of our model and its generalization ability."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 2826161408.0000 - mae: 39550.7930 - mse: 2826161408.0000 - val_loss: 2483864576.0000 - val_mae: 36633.9844 - val_mse: 2483864576.0000\n",
      "Epoch 2/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 2817560832.0000 - mae: 39352.9531 - mse: 2817560832.0000 - val_loss: 2484426496.0000 - val_mae: 36698.1055 - val_mse: 2484426496.0000\n",
      "Epoch 3/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 2782970368.0000 - mae: 39153.1211 - mse: 2782970368.0000 - val_loss: 2483450112.0000 - val_mae: 36582.3281 - val_mse: 2483450112.0000\n",
      "Epoch 4/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 2769214976.0000 - mae: 38991.2266 - mse: 2769214976.0000 - val_loss: 2477573120.0000 - val_mae: 36582.5039 - val_mse: 2477573120.0000\n",
      "Epoch 5/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 2817096960.0000 - mae: 39459.8516 - mse: 2817096960.0000 - val_loss: 2480973056.0000 - val_mae: 36540.3984 - val_mse: 2480973056.0000\n",
      "Epoch 6/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 2739611136.0000 - mae: 38814.1602 - mse: 2739611136.0000 - val_loss: 2476502784.0000 - val_mae: 36525.2188 - val_mse: 2476502784.0000\n",
      "Epoch 7/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 2814084096.0000 - mae: 39374.1953 - mse: 2814084096.0000 - val_loss: 2476084992.0000 - val_mae: 36573.9805 - val_mse: 2476084992.0000\n",
      "Epoch 8/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 2764184576.0000 - mae: 39068.5586 - mse: 2764184576.0000 - val_loss: 2474297344.0000 - val_mae: 36574.9180 - val_mse: 2474297344.0000\n",
      "Epoch 9/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 2753004544.0000 - mae: 38973.7031 - mse: 2753004544.0000 - val_loss: 2472060416.0000 - val_mae: 36482.1680 - val_mse: 2472060416.0000\n",
      "Epoch 10/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 2781382144.0000 - mae: 39248.1836 - mse: 2781382144.0000 - val_loss: 2471631872.0000 - val_mae: 36486.8477 - val_mse: 2471631872.0000\n",
      "Epoch 11/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 2786693120.0000 - mae: 39219.4062 - mse: 2786693120.0000 - val_loss: 2471078656.0000 - val_mae: 36505.8047 - val_mse: 2471078656.0000\n",
      "Epoch 12/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 2798931712.0000 - mae: 39106.5430 - mse: 2798931712.0000 - val_loss: 2468473088.0000 - val_mae: 36515.9727 - val_mse: 2468473088.0000\n",
      "Epoch 13/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 2735427072.0000 - mae: 38857.9297 - mse: 2735427072.0000 - val_loss: 2470499328.0000 - val_mae: 36607.0703 - val_mse: 2470499328.0000\n",
      "Epoch 14/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 2757617152.0000 - mae: 39060.7266 - mse: 2757617152.0000 - val_loss: 2468462848.0000 - val_mae: 36568.7148 - val_mse: 2468462848.0000\n",
      "Epoch 15/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 2751998464.0000 - mae: 38831.6953 - mse: 2751998464.0000 - val_loss: 2468982528.0000 - val_mae: 36517.1289 - val_mse: 2468982528.0000\n",
      "Epoch 16/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 2778251008.0000 - mae: 39099.1914 - mse: 2778251008.0000 - val_loss: 2470288896.0000 - val_mae: 36480.9766 - val_mse: 2470288896.0000\n",
      "Epoch 17/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 2762395648.0000 - mae: 39116.0039 - mse: 2762395648.0000 - val_loss: 2469429248.0000 - val_mae: 36556.7578 - val_mse: 2469429248.0000\n",
      "Epoch 18/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 2769951488.0000 - mae: 39119.1367 - mse: 2769951488.0000 - val_loss: 2463871744.0000 - val_mae: 36456.3828 - val_mse: 2463871744.0000\n",
      "Epoch 19/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 2771885056.0000 - mae: 39112.6562 - mse: 2771885056.0000 - val_loss: 2465443072.0000 - val_mae: 36391.6289 - val_mse: 2465443072.0000\n",
      "Epoch 20/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 2752576768.0000 - mae: 38791.6602 - mse: 2752576768.0000 - val_loss: 2461677056.0000 - val_mae: 36419.6914 - val_mse: 2461677056.0000\n",
      "Epoch 21/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 2760424960.0000 - mae: 38968.8672 - mse: 2760424960.0000 - val_loss: 2462027264.0000 - val_mae: 36363.5391 - val_mse: 2462027264.0000\n",
      "Epoch 22/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 2752374784.0000 - mae: 39019.8633 - mse: 2752374784.0000 - val_loss: 2462592512.0000 - val_mae: 36447.9375 - val_mse: 2462592512.0000\n",
      "Epoch 23/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 2762532096.0000 - mae: 38953.5977 - mse: 2762532096.0000 - val_loss: 2462567936.0000 - val_mae: 36420.5664 - val_mse: 2462567936.0000\n",
      "Epoch 24/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 2772422144.0000 - mae: 39048.9297 - mse: 2772422144.0000 - val_loss: 2461246208.0000 - val_mae: 36384.0273 - val_mse: 2461246208.0000\n",
      "Epoch 25/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 2798795264.0000 - mae: 39334.2461 - mse: 2798795264.0000 - val_loss: 2458923520.0000 - val_mae: 36415.7734 - val_mse: 2458923520.0000\n",
      "Epoch 26/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 2753320704.0000 - mae: 38953.5469 - mse: 2753320704.0000 - val_loss: 2457238528.0000 - val_mae: 36400.1562 - val_mse: 2457238528.0000\n",
      "Epoch 27/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 2745962752.0000 - mae: 38976.7383 - mse: 2745962752.0000 - val_loss: 2456145152.0000 - val_mae: 36319.0000 - val_mse: 2456145152.0000\n",
      "Epoch 28/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 2752249856.0000 - mae: 38979.2891 - mse: 2752249856.0000 - val_loss: 2456976128.0000 - val_mae: 36290.6289 - val_mse: 2456976128.0000\n",
      "Epoch 29/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 2760138240.0000 - mae: 38733.7773 - mse: 2760138240.0000 - val_loss: 2455470336.0000 - val_mae: 36369.7344 - val_mse: 2455470336.0000\n",
      "Epoch 30/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 2782192896.0000 - mae: 39039.9688 - mse: 2782192896.0000 - val_loss: 2457488384.0000 - val_mae: 36353.6172 - val_mse: 2457488384.0000\n",
      "Epoch 31/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 2711909376.0000 - mae: 38658.9102 - mse: 2711909376.0000 - val_loss: 2457134848.0000 - val_mae: 36435.3750 - val_mse: 2457134848.0000\n",
      "Epoch 32/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 2780845056.0000 - mae: 39194.8281 - mse: 2780845056.0000 - val_loss: 2456812544.0000 - val_mae: 36328.7461 - val_mse: 2456812544.0000\n",
      "Epoch 33/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 2741251328.0000 - mae: 38849.4180 - mse: 2741251328.0000 - val_loss: 2452923648.0000 - val_mae: 36329.7109 - val_mse: 2452923648.0000\n",
      "Epoch 34/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 2735047424.0000 - mae: 38820.0391 - mse: 2735047424.0000 - val_loss: 2453529856.0000 - val_mae: 36309.9766 - val_mse: 2453529856.0000\n",
      "Epoch 35/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 2717519104.0000 - mae: 38683.2656 - mse: 2717519104.0000 - val_loss: 2454628096.0000 - val_mae: 36329.0273 - val_mse: 2454628096.0000\n",
      "Epoch 36/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 2764623104.0000 - mae: 38896.8633 - mse: 2764623104.0000 - val_loss: 2451750656.0000 - val_mae: 36227.4336 - val_mse: 2451750656.0000\n",
      "Epoch 37/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 2761120512.0000 - mae: 39062.2461 - mse: 2761120512.0000 - val_loss: 2447637504.0000 - val_mae: 36287.0742 - val_mse: 2447637504.0000\n",
      "Epoch 38/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 2749396736.0000 - mae: 38978.5547 - mse: 2749396736.0000 - val_loss: 2447005440.0000 - val_mae: 36274.0938 - val_mse: 2447005440.0000\n",
      "Epoch 39/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 2718330368.0000 - mae: 38790.8594 - mse: 2718330368.0000 - val_loss: 2448241152.0000 - val_mae: 36288.0234 - val_mse: 2448241152.0000\n",
      "Epoch 40/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 2732129792.0000 - mae: 38707.2773 - mse: 2732129792.0000 - val_loss: 2447272448.0000 - val_mae: 36341.8945 - val_mse: 2447272448.0000\n",
      "Epoch 41/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 2740141056.0000 - mae: 38917.6562 - mse: 2740141056.0000 - val_loss: 2444115712.0000 - val_mae: 36216.3750 - val_mse: 2444115712.0000\n",
      "Epoch 42/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 2716115968.0000 - mae: 38643.4102 - mse: 2716115968.0000 - val_loss: 2446962176.0000 - val_mae: 36294.9766 - val_mse: 2446962176.0000\n",
      "Epoch 43/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 2783292928.0000 - mae: 39099.0547 - mse: 2783292928.0000 - val_loss: 2444806912.0000 - val_mae: 36261.7500 - val_mse: 2444806912.0000\n",
      "Epoch 44/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 2714777600.0000 - mae: 38586.5078 - mse: 2714777600.0000 - val_loss: 2445851392.0000 - val_mae: 36280.5508 - val_mse: 2445851392.0000\n",
      "Epoch 45/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 2734806784.0000 - mae: 38883.6445 - mse: 2734806784.0000 - val_loss: 2443636736.0000 - val_mae: 36308.6719 - val_mse: 2443636736.0000\n",
      "Epoch 46/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 2730926336.0000 - mae: 38735.3164 - mse: 2730926336.0000 - val_loss: 2442429440.0000 - val_mae: 36251.5703 - val_mse: 2442429440.0000\n",
      "Epoch 47/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 2731234560.0000 - mae: 38761.8164 - mse: 2731234560.0000 - val_loss: 2445441792.0000 - val_mae: 36352.0586 - val_mse: 2445441792.0000\n",
      "Epoch 48/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 2708590336.0000 - mae: 38639.3711 - mse: 2708590336.0000 - val_loss: 2443920128.0000 - val_mae: 36178.1641 - val_mse: 2443920128.0000\n",
      "Epoch 49/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 2702901248.0000 - mae: 38462.3711 - mse: 2702901248.0000 - val_loss: 2437928704.0000 - val_mae: 36165.4648 - val_mse: 2437928704.0000\n",
      "Epoch 50/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 2733915392.0000 - mae: 38768.4688 - mse: 2733915392.0000 - val_loss: 2442891520.0000 - val_mae: 36088.5859 - val_mse: 2442891520.0000\n",
      "Epoch 51/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 2732734208.0000 - mae: 38768.2344 - mse: 2732734208.0000 - val_loss: 2439809280.0000 - val_mae: 36148.1914 - val_mse: 2439809280.0000\n",
      "Epoch 52/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 2721193472.0000 - mae: 38645.4609 - mse: 2721193472.0000 - val_loss: 2437129216.0000 - val_mae: 36144.0469 - val_mse: 2437129216.0000\n",
      "Epoch 53/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 2735462912.0000 - mae: 38859.5156 - mse: 2735462912.0000 - val_loss: 2436845824.0000 - val_mae: 36126.2930 - val_mse: 2436845824.0000\n",
      "Epoch 54/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 2704766976.0000 - mae: 38719.1641 - mse: 2704766976.0000 - val_loss: 2435445760.0000 - val_mae: 36210.9922 - val_mse: 2435445760.0000\n",
      "Epoch 55/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 2753620224.0000 - mae: 39022.0508 - mse: 2753620224.0000 - val_loss: 2433048576.0000 - val_mae: 36092.3320 - val_mse: 2433048576.0000\n",
      "Epoch 56/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 2713661696.0000 - mae: 38564.8008 - mse: 2713661696.0000 - val_loss: 2437097728.0000 - val_mae: 36159.5703 - val_mse: 2437097728.0000\n",
      "Epoch 57/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 2702177792.0000 - mae: 38445.5742 - mse: 2702177792.0000 - val_loss: 2434129664.0000 - val_mae: 36179.1523 - val_mse: 2434129664.0000\n",
      "Epoch 58/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 2697528576.0000 - mae: 38675.2930 - mse: 2697528576.0000 - val_loss: 2432697856.0000 - val_mae: 36119.3320 - val_mse: 2432697856.0000\n",
      "Epoch 59/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 2761475072.0000 - mae: 38888.8320 - mse: 2761475072.0000 - val_loss: 2432203776.0000 - val_mae: 36181.7656 - val_mse: 2432203776.0000\n",
      "Epoch 60/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 2701062656.0000 - mae: 38494.0117 - mse: 2701062656.0000 - val_loss: 2435612672.0000 - val_mae: 36216.0000 - val_mse: 2435612672.0000\n",
      "Epoch 61/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 2717971712.0000 - mae: 38701.0664 - mse: 2717971712.0000 - val_loss: 2432508416.0000 - val_mae: 36130.5859 - val_mse: 2432508416.0000\n",
      "Epoch 62/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 2721430272.0000 - mae: 38620.4297 - mse: 2721430272.0000 - val_loss: 2434852608.0000 - val_mae: 36145.1250 - val_mse: 2434852608.0000\n",
      "Epoch 63/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 2690824448.0000 - mae: 38678.9375 - mse: 2690824448.0000 - val_loss: 2432054528.0000 - val_mae: 36108.3750 - val_mse: 2432054528.0000\n",
      "Epoch 64/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 2721366016.0000 - mae: 38672.6094 - mse: 2721366016.0000 - val_loss: 2430309632.0000 - val_mae: 36094.3906 - val_mse: 2430309632.0000\n",
      "Epoch 65/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 2717827072.0000 - mae: 38637.2969 - mse: 2717827072.0000 - val_loss: 2432114176.0000 - val_mae: 36158.7617 - val_mse: 2432114176.0000\n",
      "Epoch 66/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 2748510208.0000 - mae: 38864.8906 - mse: 2748510208.0000 - val_loss: 2432066560.0000 - val_mae: 36120.7266 - val_mse: 2432066560.0000\n",
      "Epoch 67/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 2669649152.0000 - mae: 38328.6680 - mse: 2669649152.0000 - val_loss: 2430569728.0000 - val_mae: 36144.8906 - val_mse: 2430569728.0000\n",
      "Epoch 68/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 2736871168.0000 - mae: 38778.2109 - mse: 2736871168.0000 - val_loss: 2430514176.0000 - val_mae: 36019.9688 - val_mse: 2430514176.0000\n",
      "Epoch 69/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 2697245440.0000 - mae: 38501.1289 - mse: 2697245440.0000 - val_loss: 2428474880.0000 - val_mae: 35997.0625 - val_mse: 2428474880.0000\n",
      "Epoch 70/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 2716807680.0000 - mae: 38555.6016 - mse: 2716807680.0000 - val_loss: 2428622592.0000 - val_mae: 36130.7812 - val_mse: 2428622592.0000\n",
      "Epoch 71/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 2694615552.0000 - mae: 38626.9453 - mse: 2694615552.0000 - val_loss: 2430393344.0000 - val_mae: 36109.6953 - val_mse: 2430393344.0000\n",
      "Epoch 72/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 2692846080.0000 - mae: 38351.5078 - mse: 2692846080.0000 - val_loss: 2428061696.0000 - val_mae: 36002.6445 - val_mse: 2428061696.0000\n",
      "Epoch 73/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 2706827008.0000 - mae: 38690.9219 - mse: 2706827008.0000 - val_loss: 2427742720.0000 - val_mae: 36147.6680 - val_mse: 2427742720.0000\n",
      "Epoch 74/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 2749847552.0000 - mae: 38864.5469 - mse: 2749847552.0000 - val_loss: 2425128704.0000 - val_mae: 36002.9727 - val_mse: 2425128704.0000\n",
      "Epoch 75/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 2727920640.0000 - mae: 38767.3828 - mse: 2727920640.0000 - val_loss: 2425922304.0000 - val_mae: 35950.4961 - val_mse: 2425922304.0000\n",
      "Epoch 76/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 2728878592.0000 - mae: 38698.8438 - mse: 2728878592.0000 - val_loss: 2428389888.0000 - val_mae: 36127.5547 - val_mse: 2428389888.0000\n",
      "Epoch 77/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 2722894080.0000 - mae: 38799.8789 - mse: 2722894080.0000 - val_loss: 2422649856.0000 - val_mae: 36061.9609 - val_mse: 2422649856.0000\n",
      "Epoch 78/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 2711745792.0000 - mae: 38795.3320 - mse: 2711745792.0000 - val_loss: 2427677952.0000 - val_mae: 36098.0586 - val_mse: 2427677952.0000\n",
      "Epoch 79/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 2694706176.0000 - mae: 38633.8867 - mse: 2694706176.0000 - val_loss: 2425571584.0000 - val_mae: 35957.4492 - val_mse: 2425571584.0000\n",
      "Epoch 80/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 2721555456.0000 - mae: 38661.9922 - mse: 2721555456.0000 - val_loss: 2422975232.0000 - val_mae: 36036.6992 - val_mse: 2422975232.0000\n",
      "Epoch 81/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 2718827520.0000 - mae: 38581.7305 - mse: 2718827520.0000 - val_loss: 2425799936.0000 - val_mae: 36004.9570 - val_mse: 2425799936.0000\n",
      "Epoch 82/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 2713930240.0000 - mae: 38572.9766 - mse: 2713930240.0000 - val_loss: 2422398720.0000 - val_mae: 36012.9062 - val_mse: 2422398720.0000\n",
      "Epoch 83/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 2670513408.0000 - mae: 38475.8945 - mse: 2670513408.0000 - val_loss: 2422035200.0000 - val_mae: 36054.2227 - val_mse: 2422035200.0000\n",
      "Epoch 84/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 2746643968.0000 - mae: 38898.2344 - mse: 2746643968.0000 - val_loss: 2421470464.0000 - val_mae: 36009.6836 - val_mse: 2421470464.0000\n",
      "Epoch 85/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 2684621056.0000 - mae: 38508.5938 - mse: 2684621056.0000 - val_loss: 2423088640.0000 - val_mae: 35994.3984 - val_mse: 2423088640.0000\n",
      "Epoch 86/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 2718481920.0000 - mae: 38782.4258 - mse: 2718481920.0000 - val_loss: 2420483072.0000 - val_mae: 35998.0195 - val_mse: 2420483072.0000\n",
      "Epoch 87/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 2717836544.0000 - mae: 38686.3398 - mse: 2717836544.0000 - val_loss: 2419297536.0000 - val_mae: 36017.1445 - val_mse: 2419297536.0000\n",
      "Epoch 88/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 2707889152.0000 - mae: 38616.0039 - mse: 2707889152.0000 - val_loss: 2419105792.0000 - val_mae: 36047.7695 - val_mse: 2419105792.0000\n",
      "Epoch 89/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 2736000256.0000 - mae: 38793.4102 - mse: 2736000256.0000 - val_loss: 2420257536.0000 - val_mae: 35965.1797 - val_mse: 2420257536.0000\n",
      "Epoch 90/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 2701773824.0000 - mae: 38621.2461 - mse: 2701773824.0000 - val_loss: 2421417984.0000 - val_mae: 36109.4570 - val_mse: 2421417984.0000\n",
      "Epoch 91/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 2710966016.0000 - mae: 38719.3516 - mse: 2710966016.0000 - val_loss: 2422517248.0000 - val_mae: 36069.2109 - val_mse: 2422517248.0000\n",
      "Epoch 92/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 2736283904.0000 - mae: 38753.9766 - mse: 2736283904.0000 - val_loss: 2418155776.0000 - val_mae: 36043.0820 - val_mse: 2418155776.0000\n",
      "Epoch 93/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 2670017536.0000 - mae: 38246.6602 - mse: 2670017536.0000 - val_loss: 2418381056.0000 - val_mae: 36089.0312 - val_mse: 2418381056.0000\n",
      "Epoch 94/100\n",
      "315/315 [==============================] - 1s 3ms/step - loss: 2722990848.0000 - mae: 38703.7305 - mse: 2722990848.0000 - val_loss: 2421978112.0000 - val_mae: 36034.8906 - val_mse: 2421978112.0000\n",
      "Epoch 95/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 2687016192.0000 - mae: 38429.0039 - mse: 2687016192.0000 - val_loss: 2418318848.0000 - val_mae: 35941.3945 - val_mse: 2418318848.0000\n",
      "Epoch 96/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 2717611776.0000 - mae: 38630.9023 - mse: 2717611776.0000 - val_loss: 2419029760.0000 - val_mae: 36095.1055 - val_mse: 2419029760.0000\n",
      "Epoch 97/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 2696225536.0000 - mae: 38391.7188 - mse: 2696225536.0000 - val_loss: 2415296512.0000 - val_mae: 35970.9844 - val_mse: 2415296512.0000\n",
      "Epoch 98/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 2695559680.0000 - mae: 38403.1289 - mse: 2695559680.0000 - val_loss: 2416170240.0000 - val_mae: 35977.7695 - val_mse: 2416170240.0000\n",
      "Epoch 99/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 2724553728.0000 - mae: 38631.0625 - mse: 2724553728.0000 - val_loss: 2416973568.0000 - val_mae: 35960.0625 - val_mse: 2416973568.0000\n",
      "Epoch 100/100\n",
      "315/315 [==============================] - 1s 2ms/step - loss: 2696665088.0000 - mae: 38488.1406 - mse: 2696665088.0000 - val_loss: 2415644160.0000 - val_mae: 35998.3125 - val_mse: 2415644160.0000\n"
     ]
    }
   ],
   "source": [
    "# Continue training the model\n",
    "history = model.fit(X_train, y_train, epochs=100, validation_split = 0.2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Finally, we save our improved model for future use."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# Save the model\n",
    "model.save('housing_model_improved.h5')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Summary\n",
    "In this notebook, we've built upon our preprocessed data, constructed both a baseline linear regression model and a deep learning model, and made attempts to improve the latter. We've also trained our models and evaluated their performance, giving us a good idea of how well they can predict house prices. In the next notebook, we'll delve into model evaluation and optimization."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}